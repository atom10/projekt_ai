services:
  ollama:
    build:
        dockerfile: dockerfiles/ollama
        context: ./
    container_name: ollama
    environment:
      - LLM_MODEL=${LLM_MODEL}
    #image: ollama/ollama
    #entrypoint: ["sh", "-c", "ollama pull ${LLM_MODEL}; ollama serve"]
    entrypoint: "/tmp/run-ollama.sh"
    tty: true
    healthcheck:
      test: ollama --version || exit 1
      interval: 10s
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    restart: always
    pull_policy: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  app:
    build:
      context: .
      dockerfile: dockerfiles/python
    container_name: python_app
    environment:
      - LLM_MODEL=${LLM_MODEL}
    volumes:
      - ./app:/app
    stdin_open: true  # Keeps stdin open even if not attached
    tty: true         # Allocate a pseudo-TTY
    depends_on:
      - ollama

  # open-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   environment:
  #     - WEBUI_AUTH=False
  #     - 'OLLAMA_BASE_URL=http://ollama:11434'
  #     - 'WEBUI_SECRET_KEY='
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   ports:
  #     - "8080:8080"
  #   volumes:
  #     - ./open-webui:/app/backend/data
  #   networks:
  #     - default
  #   restart: always
  #   depends_on:
  #     - ollama

networks:
  default:
